{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b3a43c-7e36-4bff-ad86-517f4da60e24",
   "metadata": {},
   "source": [
    "Assignment Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f33dd6f-f112-4ca0-847d-452b35c98285",
   "metadata": {},
   "source": [
    "Theoretical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50523850-9ed4-480b-9c93-a1ab279e303a",
   "metadata": {},
   "source": [
    "1).What is Logistic Regression, and how does it differ from Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d35a0f-b118-44cd-88a0-1a5faa2353d6",
   "metadata": {},
   "source": [
    "    Linear Regression:\n",
    "\n",
    "Purpose:\n",
    "Used to predict a continuous numerical output. For example, predicting house prices, temperature, or sales figures.   \n",
    "It models the relationship between independent variables and a dependent variable by fitting a straight line to the data.   \n",
    "\n",
    "Output:\n",
    "Produces a continuous value.   \n",
    "\n",
    "Use Case:\n",
    "When you want to estimate a quantity.\n",
    "\n",
    "    Logistic Regression:\n",
    "\n",
    "Purpose:\n",
    "Used for classification problems, where the goal is to predict a categorical outcome. Often, this is a binary classification (e.g., yes/no, true/false). For example, predicting whether an email is spam or not, or whether a customer will click on an ad.   \n",
    "It models the probability of a certain class or event occurring.   \n",
    "\n",
    "Output:\n",
    "Produces a probability value, which is then often used to classify data into categories.   \n",
    "\n",
    "Use Case:\n",
    "When you want to classify data into distinct categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079e9cf-2a80-4a30-8291-4811eaf67f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926ac480-c9dd-453e-8ddf-5a56a1f5e372",
   "metadata": {},
   "source": [
    "2). What is the mathematical equation of Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371ccf8-128c-464e-b6f0-75fcd1fea10a",
   "metadata": {},
   "source": [
    "1. Linear Component:\n",
    "\n",
    "Like linear regression, logistic regression starts with a linear combination of input features:\n",
    " \"z = β₀ + β₁x₁ + β₂x₂ + ... + βnxn\"\n",
    "\n",
    "Where:\n",
    "z is the linear combination.\n",
    "β₀ is the intercept.\n",
    "β₁, β₂, ..., βn are the coefficients for the features.\n",
    "x₁, x₂, ..., xn are the input features.\n",
    "  \n",
    "\n",
    "2. Sigmoid Function:\n",
    "\n",
    "However, logistic regression aims to predict probabilities, which must be between 0 and 1. To achieve this, it applies the sigmoid function (also called the logistic function) to the linear combination:\n",
    " \"σ(z) = 1 / (1 + e^(-z))\" \n",
    "\n",
    "Where:\n",
    "σ(z) is the probability of the outcome being 1.\n",
    "e is Euler's number (approximately 2.71828).\n",
    "  \n",
    "\n",
    "3. Combining the Equations:\n",
    "\n",
    "By substituting the linear component into the sigmoid function, we get the complete logistic regression equation:\n",
    " \"P(y=1) = 1 / (1 + e^(-(β₀ + β₁x₁ + β₂x₂ + ... + βnxn)))\" \n",
    "\n",
    "Where:\n",
    "P(y=1) is the probability that the dependent variable y is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd6138-b5ce-4b2c-9655-9ee02191d2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6001b49-6f19-40ae-8da9-ee0b61cd0134",
   "metadata": {},
   "source": [
    "3). Why do we use the Sigmoid function in Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7b42f-17f1-4604-9050-8d6865a6bc0a",
   "metadata": {},
   "source": [
    "1. Mapping to Probabilities:\n",
    "\n",
    "Logistic regression aims to predict the probability of a data point belonging to a particular class. Probabilities, by definition, must fall within the range of 0 to 1.   \n",
    "The sigmoid function, with its \"S\"-shaped curve, conveniently maps any real number input to a value between 0 and 1. This makes it ideal for transforming the output of the linear combination of features into a probability.   \n",
    "\n",
    "2. Binary Classification:\n",
    "\n",
    "Logistic regression is frequently used for binary classification, where there are only two possible outcomes. The sigmoid function's output can be interpreted as the probability of the positive class.   \n",
    "A threshold (often 0.5) is then used to classify the data point. If the sigmoid output is greater than the threshold, the point is assigned to one class; otherwise, it's assigned to the other.   \n",
    "\n",
    "3. Mathematical Properties:\n",
    "\n",
    "The sigmoid function is differentiable, which is essential for optimization algorithms like gradient descent. These algorithms are used to find the optimal coefficients for the logistic regression model.   \n",
    "Also the shape of the sigmoid function lends itself well to showing strong distinctions between data that is easily classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4e8f6-f511-49c8-a214-a1fc46a12b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f39bf37-916e-465b-b1b3-1b03e05616b0",
   "metadata": {},
   "source": [
    "4). What is the cost function of Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32a444-19ec-418e-84aa-8cbc4c790a3a",
   "metadata": {},
   "source": [
    "In logistic regression, the goal is to find the model parameters that best predict the probability of a certain outcome. To achieve this, we need a cost function that measures how well our model is performing. Unlike linear regression, which typically uses the mean squared error, logistic regression uses a cost function called \"log loss\" or \"cross-entropy loss.\"\n",
    "\n",
    "The Log Loss (Cross-Entropy Loss) Function:\n",
    "\n",
    "The log loss function is designed to be convex, ensuring that there's a single global minimum. It's defined as follows:   \n",
    "\n",
    "For a single training example:\n",
    "\n",
    "If y = 1: Cost(h(x), y) = -log(h(x))\n",
    "\n",
    "If y = 0: Cost(h(x), y) = -log(1 - h(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0324b-7888-41c6-9035-a109f333079e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb5f14-34ba-4b06-a8a0-d86e3432dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "5). What is Regularization in Logistic Regression? Why is it needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecdeb84-fd4c-4f92-8b31-b4826613fb72",
   "metadata": {},
   "source": [
    "Regularization in logistic regression, as in other machine learning models, is a technique used to prevent overfitting. Here's a breakdown of what it is and why it's essential:   \n",
    "\n",
    "What is Regularization?\n",
    "\n",
    "Essentially, regularization involves adding a penalty term to the cost function of the logistic regression model. This penalty discourages the model from assigning excessively large coefficients to the features.   \n",
    "By constraining the size of the coefficients, regularization helps to simplify the model, making it less prone to fitting the noise in the training data\n",
    "\n",
    "Why is it Needed?\n",
    "\n",
    "Overfitting:\n",
    "\n",
    "Improving Generalization:\n",
    "\n",
    "Handling Multicollinearity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105f1ee-f094-4ad7-9ea9-ebb665088e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0d719d5-81f1-4e8e-9b79-d0c416d6ace6",
   "metadata": {},
   "source": [
    "6). Explain the difference between Lasso, Ridge, and Elastic Net regressionC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3987c-64e7-4131-93c9-0c9578d5d81c",
   "metadata": {},
   "source": [
    "When dealing with linear regression models, especially in situations with many features or potential overfitting, regularization becomes crucial. Lasso, Ridge, and Elastic Net are three popular regularization techniques, each with its own approach. Here's a breakdown of their differences:   \n",
    "\n",
    "1. Ridge Regression (L2 Regularization):\n",
    "\n",
    "Penalty:\n",
    "\n",
    "Adds a penalty term proportional to the square of the magnitude of the coefficients.   \n",
    "\n",
    "Effect:\n",
    "\n",
    "Shrinks the coefficients towards zero, but rarely forces them to be exactly zero.   \n",
    "Reduces the impact of less important features, but retains all of them in the model.   \n",
    "Effective in reducing multicollinearity (high correlation between features).   \n",
    "\n",
    "Use Case:\n",
    "\n",
    "Useful when you have many features, and you believe most of them are relevant.\n",
    "Good for preventing overfitting when you want to keep all features.\n",
    "\n",
    "2. Lasso Regression (L1 Regularization):\n",
    "\n",
    "Penalty:\n",
    "\n",
    "Adds a penalty term proportional to the absolute value of the magnitude of the coefficients.   \n",
    "\n",
    "Effect:\n",
    "\n",
    "Can shrink some coefficients to exactly zero, effectively performing feature selection.   \n",
    "Creates a sparse model, meaning it includes only the most important features.   \n",
    "Useful for simplifying the model and improving interpretability.   \n",
    "\n",
    "Use Case:\n",
    "\n",
    "Ideal when you suspect that only a subset of features is truly important.\n",
    "Helpful for automatic feature selection.   \n",
    "\n",
    "3. Elastic Net Regression:\n",
    "\n",
    "Penalty:\n",
    "\n",
    "Combines both L1 and L2 penalties. It's a linear combination of the Lasso and Ridge penalties.   \n",
    "\n",
    "Effect:\n",
    "\n",
    "Provides a balance between the feature selection of Lasso and the coefficient shrinkage of Ridge.   \n",
    "Can handle situations where there are highly correlated features more effectively than Lasso alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4453978-48ae-46f2-a9bd-9a918f5bbad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad96eaa3-5c03-4c0e-9421-9795f60baaee",
   "metadata": {},
   "source": [
    "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
    "\n",
    "High Feature Correlation:\n",
    "\n",
    "When you have many features, and some of them are highly correlated, Elastic Net tends to perform better than Lasso. Lasso might arbitrarily select one feature from a group of correlated features and discard the others, while Elastic Net will typically select groups of correlated features.   \n",
    "\n",
    "Balancing Feature Selection and Coefficient Shrinkage:\n",
    "\n",
    "If you want to combine the feature selection capabilities of Lasso with the coefficient shrinkage of Ridge, Elastic Net is a good choice. It provides a balance between these two approaches.   \n",
    "\n",
    "Uncertainty about Feature Importance:\n",
    "\n",
    "When you're unsure whether all features are relevant or if only a subset is important, Elastic Net offers a robust approach. It helps to reduce the impact of less important features while still retaining potentially valuable ones.\n",
    "\n",
    "When Lasso is too sensitive to the data:\n",
    "\n",
    "Lasso can be very sensative to the data, and may have high variance. Elastic net reduces this variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b1477-b18c-42ee-a6c6-eb6dc29d6018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ac2958-3bea-4ce3-a230-546626898109",
   "metadata": {},
   "source": [
    "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
    "\n",
    "λ (Lambda) Controls Regularization Strength:\n",
    "\n",
    "λ is the regularization parameter that controls the strength of the penalty term.   \n",
    "\n",
    "Higher λ:\n",
    "\n",
    "A higher λ value increases the penalty, leading to stronger regularization. This results in smaller coefficients, simpler models, and a reduced risk of overfitting. However, too high a λ can lead to underfitting.   \n",
    "\n",
    "Lower λ:\n",
    "\n",
    "A lower λ value reduces the penalty, resulting in weaker regularization. This allows the model to fit the training data more closely, but it increases the risk of overfitting.   \n",
    "λ = 0:\n",
    "When λ is 0, there is no regularization, and the model behaves like a standard logistic regression without any penalty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8955c7-fbbf-4351-b5e7-57856569dc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71b1f6a0-5c6d-436b-b8a2-4357e25afc02",
   "metadata": {},
   "source": [
    "9. What are the key assumptions of Logistic Regression?\n",
    "\n",
    "Binary or Multiclass Output:\n",
    "\n",
    "Logistic regression is designed for classification problems with a categorical dependent variable.   \n",
    "\n",
    "Linearity in the Logit:\n",
    "\n",
    "The relationship between the independent variables and the log-odds (logit) of the outcome is assumed to be linear.   \n",
    "\n",
    "Independence of Observations:\n",
    "\n",
    "The observations are assumed to be independent of each other.\n",
    "\n",
    "No Strong Multicollinearity:\n",
    "\n",
    "The independent variables should not be highly correlated with each other.\n",
    "\n",
    "Sufficiently Large Sample Size:\n",
    "\n",
    "Logistic regression generally requires a sufficiently large sample size for reliable results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3b418-2889-45ab-817d-7b22865361ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d14ec38b-0dd0-4268-9cdd-a7cbfec914bb",
   "metadata": {},
   "source": [
    "10. What are some alternatives to Logistic Regression for classification tasks?\n",
    "\n",
    "Decision Trees:\n",
    "\n",
    "Tree-based models that partition the data based on feature values.\n",
    "\n",
    "Random Forests:\n",
    "\n",
    "Ensemble of decision trees that improve accuracy and reduce overfitting.\n",
    "\n",
    "Support Vector Machines (SVMs):\n",
    "\n",
    "Models that find the optimal hyperplane to separate classes.\n",
    "\n",
    "K-Nearest Neighbors (KNN):\n",
    "\n",
    "Classifies data points based on the majority class of their nearest neighbors.\n",
    "\n",
    "Naive Bayes:\n",
    "\n",
    "Probabilistic models based on Bayes' theorem.\n",
    "\n",
    "Neural Networks:\n",
    "\n",
    "Complex models that can learn intricate patterns in the data.\n",
    "\n",
    "Gradient Boosting Machines (GBM):\n",
    "\n",
    "Ensemble learning method that builds trees in a sequential manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46611af3-b9ce-4f2f-8526-d89d1d2c4d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0015b88e-638e-4f45-8186-82e80921413a",
   "metadata": {},
   "source": [
    "11. What are Classification Evaluation Metrics?\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "The proportion of correctly classified instances.\n",
    "\n",
    "Precision:\n",
    "\n",
    "The proportion of correctly predicted positive instances out of all predicted positives.\n",
    "\n",
    "Recall (Sensitivity):\n",
    "\n",
    "The proportion of correctly predicted positive instances out of all actual positives.   \n",
    "\n",
    "F1-Score:\n",
    "\n",
    "The harmonic mean of precision and recall.\n",
    "\n",
    "AUC-ROC (Area Under the Receiver Operating Characteristic Curve):\n",
    "\n",
    "Measures the model's ability to distinguish between classes.\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "A table that summarizes the model's performance by showing true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Log Loss (Cross-Entropy Loss):\n",
    "\n",
    "Measures the performance of a classification model where the prediction input is a probability value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d0b7e-7793-4f7d-b250-0cf4d0900b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9a441db-8b45-4ce4-91a3-ecfcae6a7210",
   "metadata": {},
   "source": [
    "12. How does class imbalance affect Logistic Regression?\n",
    "\n",
    "Bias Towards Majority Class:\n",
    "\n",
    "Class imbalance occurs when one class has significantly more instances than the other. Logistic regression tends to be biased towards the majority class, leading to poor performance on the minority class.   \n",
    "\n",
    "Misleading Accuracy:\n",
    "\n",
    "Accuracy can be misleading when there is class imbalance, as the model can achieve high accuracy by simply predicting the majority class for all instances.\n",
    "\n",
    "Poor Recall and Precision:\n",
    "\n",
    "The model may have low recall and precision for the minority class, as it struggles to correctly identify instances of that class.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Oversampling the minority class.\n",
    "Undersampling the majority class.\n",
    "Using class weights to penalize misclassifications of the minority class.   \n",
    "Using evaluation metrics like F1-score, AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace8390-baa2-4120-9f1f-7e3d4f2cc51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be87ffe5-ab25-4495-bfb2-66df0e2d880b",
   "metadata": {},
   "source": [
    "13. What is Hyperparameter Tuning in Logistic Regression?\n",
    "\n",
    "Finding Optimal Parameters:\n",
    "\n",
    "Hyperparameter tuning involves finding the optimal values for the parameters that control the learning process of the logistic regression model, such as the regularization parameter (λ) and the solver.   \n",
    "\n",
    "Improving Model Performance:\n",
    "\n",
    "By tuning hyperparameters, you can improve the model's performance, reduce overfitting, and enhance its generalization ability.   \n",
    "\n",
    "Techniques:\n",
    "\n",
    "Grid search: Exhaustively searches through a predefined set of hyperparameter values.   \n",
    "\n",
    "Random search: Randomly samples hyperparameter values.\n",
    "\n",
    "Cross-validation: Evaluates the model's performance on multiple subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c12ee-81e7-443c-af4d-67a2d20b232b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d120727-42e0-4ff5-93df-70e415ced12c",
   "metadata": {},
   "source": [
    "14. What are different solvers in Logistic Regression? Which one should be used?\n",
    "\n",
    "Solvers:\n",
    "\n",
    "\"liblinear\": Suitable for small datasets.   \n",
    "\n",
    "\"lbfgs\": Suitable for small to medium-sized datasets.   \n",
    "\n",
    "\"newton-cg\": Suitable for small to medium-sized datasets.   \n",
    "\n",
    "\"sag\": Suitable for large datasets.   \n",
    "\n",
    "\"saga\": Suitable for large datasets, especially when using L1 regularization.   \n",
    "\n",
    "    Which One to Use:\n",
    "\n",
    "For small datasets, \"liblinear\" is often a good choice.\n",
    "\n",
    "For medium-sized datasets, \"lbfgs\" or \"newton-cg\" are usually reliable.\n",
    "\n",
    "For large datasets, \"sag\" or \"saga\" are preferred. \"saga\" is very good when using L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f540a-a4b4-4798-a004-a658a562eb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30d6f8a4-c2c9-4dda-8ff1-cb0853e421a5",
   "metadata": {},
   "source": [
    "15. How is Logistic Regression extended for multiclass classification?\n",
    "\n",
    "One-vs-Rest (OvR) or One-vs-All (OvA):\n",
    "\n",
    "Trains multiple binary logistic regression classifiers, one for each class versus all other classes.\n",
    "\n",
    "Softmax Regression (Multinomial Logistic Regression):\n",
    "\n",
    "Generalizes logistic regression to handle multiple classes directly by using the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb078485-41d1-4142-be79-fb0377a8fe5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a31aa18-4ab4-4311-bc86-03f65c89ee20",
   "metadata": {},
   "source": [
    "16. What are the advantages and disadvantages of Logistic Regression?\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Easy to implement and interpret.\n",
    "\n",
    "Efficient to train.\n",
    "\n",
    "Provides probability estimates.\n",
    "\n",
    "Works well for linearly separable data.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Assumes a linear relationship between features and the log-odds.\n",
    "\n",
    "Can struggle with complex non-linear relationships.\n",
    "\n",
    "Sensitive to outliers.\n",
    "\n",
    "Can overfit with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd120ff-e091-44f2-9cd8-4fd5f31edc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5874a30-5777-4a2a-aabf-3d92c07a7ab9",
   "metadata": {},
   "source": [
    "17. What are some use cases of Logistic Regression?\n",
    "\n",
    "Spam Detection:\n",
    "\n",
    "Classifying emails as spam or not spam.\n",
    "\n",
    "Medical Diagnosis:\n",
    "\n",
    "Predicting the presence or absence of a disease.\n",
    "\n",
    "Credit Risk Assessment:\n",
    "\n",
    "Predicting the likelihood of a customer defaulting on a loan.\n",
    "\n",
    "Customer Churn Prediction:\n",
    "\n",
    "Predicting whether a customer will stop using a service.\n",
    "\n",
    "Online Advertising:\n",
    "\n",
    "Predicting whether a user will click on an ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62e6ec-e142-47c5-ba79-e5cbc819af0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a7f9af9-4a52-4883-9f01-01e37f381f46",
   "metadata": {},
   "source": [
    "18). What is the difference between Softmax Regression and Logistic Regression.\n",
    "\n",
    "    Logistic Regression:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Designed for binary classification problems. It deals with situations where the outcome can be one of two possible classes (e.g., yes/no, true/false, 0/1).   \n",
    "\n",
    "Output:\n",
    "\n",
    "Produces a probability between 0 and 1, representing the likelihood of an instance belonging to one of the two classes.   \n",
    "\n",
    "Function:\n",
    "\n",
    "Uses the sigmoid function to transform the linear combination of features into a probability.   \n",
    "\n",
    "Use Case:\n",
    "\n",
    "Predicting whether an email is spam or not, whether a customer will click on an ad, or whether a patient has a certain disease.   \n",
    "\n",
    "    Softmax Regression (Multinomial Logistic Regression):\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Designed for multiclass classification problems. It handles situations where the outcome can be one of three or more possible classes (e.g., classifying different types of animals, identifying different digits in an image).   \n",
    "\n",
    "Output:\n",
    "\n",
    "Produces a probability distribution over all the classes, where each probability represents the likelihood of an instance belonging to a specific class.   \n",
    "\n",
    "Function:\n",
    "\n",
    "Uses the softmax function to transform the linear combination of features into a probability distribution.   \n",
    "\n",
    "Relationship to Logistic Regression:\n",
    "\n",
    "Softmax regression can be seen as a generalization of logistic regression to handle multiclass problems. When there are only two classes, softmax regression becomes equivalent to logistic regression.   \n",
    "\n",
    "Use Case:\n",
    "\n",
    "Image classification, natural language processing (e.g., classifying documents into different topics), and predicting the next word in a sentence.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34534dc5-d35f-4d2d-92f4-65410c317194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72ba4578-3187-467e-96f6-645c29b5cec7",
   "metadata": {},
   "source": [
    "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
    "\n",
    "The choice between One-vs-Rest (OvR) and Softmax Regression for multiclass classification depends on several factors:\n",
    "\n",
    "Computational Efficiency:\n",
    "\n",
    "OvR: Can be computationally less expensive when you have a large number of classes, as it trains multiple independent binary classifiers.\n",
    "\n",
    "Softmax: Can become computationally expensive with a very large number of classes, as it calculates probabilities for all classes simultaneously.   \n",
    "\n",
    "Interpretability:\n",
    "\n",
    "OvR: Provides individual binary classifiers, which can be easier to interpret in some cases.   \n",
    "\n",
    "Softmax: Provides a single model with probabilities for all classes, which can be more concise.\n",
    "\n",
    "Class Exclusivity:\n",
    "\n",
    "Softmax: Assumes that the classes are mutually exclusive (an instance belongs to only one class). This is ideal for cases like classifying images into \n",
    "\n",
    "different animal species.   \n",
    "\n",
    "OvR: Does not inherently assume class exclusivity. This can be beneficial in situations where an instance might belong to multiple classes (though this is less common in typical classification problems).\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Many libraries offer both OvR and Softmax implementations. OvR is sometimes the default for multiclass logistic regression.\n",
    "\n",
    "Performance:\n",
    "\n",
    "In many cases, Softmax and OvR will produce similar results. However, Softmax is generally preferred when the classes are mutually exclusive and you want a single, unified model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e10f3-124c-4799-a3bb-1682c08d7577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10a47285-1a3a-407c-a164-9c4994f7ce0a",
   "metadata": {},
   "source": [
    "20. How do we interpret coefficients in Logistic Regression?\n",
    "\n",
    "Interpreting coefficients in logistic regression is slightly different from linear regression due to the sigmoid function and the focus on probabilities. Here's how to approach it:   \n",
    "\n",
    "Log-Odds:\n",
    "\n",
    "The coefficients represent the change in the log-odds of the outcome variable for a one-unit change in the predictor variable, holding all other predictors constant.   \n",
    "The log-odds are the logarithm of the odds, where odds are the probability of the event occurring divided by the probability of it not occurring.   \n",
    "\n",
    "Direction of the Effect:\n",
    "\n",
    "A positive coefficient indicates that an increase in the predictor variable is associated with an increase in the log-odds of the outcome (and therefore an increase in the probability of the outcome).   \n",
    "A negative coefficient indicates that an increase in the predictor variable is associated with a decrease in the log-odds of the outcome (and therefore a decrease in the probability of the outcome).   \n",
    "\n",
    "Magnitude of the Effect:\n",
    "\n",
    "To interpret the magnitude of the effect, you can exponentiate the coefficients (e^coefficient). This gives you the odds ratio.\n",
    "An odds ratio greater than 1 indicates that the odds of the outcome increase by that factor for a one-unit increase in the predictor.   \n",
    "An odds ratio less than 1 indicates that the odds of the outcome decrease by that factor for a one-unit increase in the predictor.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8795be4-3cdc-4fb2-9ebd-8d73fb38d234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51764dcd-d8c8-4908-a1a0-bad41a1f27d0",
   "metadata": {},
   "source": [
    "        Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ca43b-d7b5-4a36-bde5-76955d8a4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "1). Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic \n",
    "Regression, and prints the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db1ce1cd-9dc1-4cd0-bbaf-607236a21098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c507cf4a-99bd-4032-b96f-13844cb8bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ad02565-d906-4666-850c-a91fa57b1fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8f253ca-9edc-46aa-a688-ae6341dc2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X_bc = breast_cancer.data\n",
    "y_bc = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34bfcf23-ffd7-4ef8-8dc3-d62be1bcb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8c06c46-b02d-44e2-af6a-be331389d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Dataset Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "model_bc = LogisticRegression(solver='lbfgs', max_iter=10000) \n",
    "model_bc.fit(X_train_bc, y_train_bc)\n",
    "\n",
    "y_pred_bc = model_bc.predict(X_test_bc)\n",
    "\n",
    "accuracy_bc = accuracy_score(y_test_bc, y_pred_bc)\n",
    "print(\"Breast Cancer Dataset Accuracy:\", accuracy_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680c06b-f8d9-40d4-ad9d-e78bcb271cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "182f1505-1140-4409-b5b3-4041fd49f0f4",
   "metadata": {},
   "source": [
    "2). Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') \n",
    "and print the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f9660a9-c207-4bd8-8611-2475974d0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f156ede3-a656-4859-b213-4e7b07ca49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80b7fb7b-b949-4044-8b42-ed6e5288110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Regularization (Lasso) Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"L1 Regularization (Lasso) Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85dddff-164a-45d4-b1bd-f6305fc5f048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d5d724-a3fa-47c4-8b7d-53ea7a3fe025",
   "metadata": {},
   "source": [
    "3). Write a Python program to train Logistic Regression with L2 regularization (Ridge) using \n",
    "LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "708efcb9-b11d-49e2-910a-c8eb4a2f0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "defef3a9-170f-4b4a-8a92-08006bbed60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69167476-a4de-48e8-a19d-265226d627d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Regularization (Ridge) Accuracy: 0.956140350877193\n",
      "Coefficients: [[ 0.98354004  0.22615743 -0.36832332  0.02630641 -0.153082   -0.23171913\n",
      "  -0.52017841 -0.27408629 -0.22286967 -0.03677268 -0.09394405  1.38710248\n",
      "  -0.16291261 -0.0891512  -0.02189262  0.04398039 -0.04602427 -0.03149534\n",
      "  -0.03407006  0.01105651  0.10038686 -0.51471036 -0.01706763 -0.01659449\n",
      "  -0.30221217 -0.76450279 -1.4116666  -0.49630847 -0.73140557 -0.10105383]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"L2 Regularization (Ridge) Accuracy:\", accuracy)\n",
    "print(\"Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75788932-26f3-4606-86de-d821f4ec54cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb73c6b2-13c1-4984-8ff5-b0ece40a1aec",
   "metadata": {},
   "source": [
    "4). Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce182e27-9d7f-416a-9067-35ba52e3e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f038f814-d337-4c02-9193-59c66a5b59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a77eda9e-36ad-497b-9300-0dbd6972423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Regularization Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Elastic Net Regularization Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa88767-e714-4d4a-8d0e-641eb67a0fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5e1e39-eecb-44eb-8e7d-ff79c0afcca3",
   "metadata": {},
   "source": [
    "5). Write a Python program to train a Logistic Regression model for multiclass classification using \n",
    "multi_class='ovr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6367311a-9c07-4ebd-b819-85a40a43bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04382191-00d0-4217-bc18-edc8697e30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0ebed01-f9c0-4233-a42a-248eb5d5e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass (OvR) Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Multiclass (OvR) Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05112c-92a2-49ec-87d7-9a51dd976287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32351c96-40cc-42a2-858c-cb82afc3c8e6",
   "metadata": {},
   "source": [
    "6). Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic \n",
    "Regression. Print the best parameters and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "77db01c3-92fb-4546-91fb-0d2266392b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.001, 'penalty': 'l2'}\n",
      "Best Model Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2', 'elasticnet']}\n",
    "model = LogisticRegression(solver='saga', max_iter=10000, random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eeaf12-273d-4193-ad85-3157b9a9a0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb0dbfe-5b64-4b1d-80bf-4cbd8e47f62e",
   "metadata": {},
   "source": [
    "7). Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the \n",
    "average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5983ac44-280f-4a90-8b56-547dbccaeae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Stratified K-Fold Accuracy: 0.9543393882937432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=10000, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X, y, cv=skf)\n",
    "\n",
    "print(\"Average Stratified K-Fold Accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a8de6-305e-4262-941e-066907ad6f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d8abeb1-7524-4a52-997a-b09be6b660d7",
   "metadata": {},
   "source": [
    "8). Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74a84f-518e-4430-ab34-5fc0b54478f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "file_path = r\"C:\\Users\\guruk\\Train.csv\"  # Raw string path\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    # Rest of your code (feature selection, train-test split, model training, etc.)\n",
    "    X = data.drop('target_column', axis=1) #replace 'target_column'\n",
    "    y = data['target_column'] #replace 'target_column'\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"CSV Dataset Accuracy:\", accuracy)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ba65b-bd4d-4d8f-96a4-96c995eadad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "103c3abb-7d0c-4e9a-9525-bf468a99f97c",
   "metadata": {},
   "source": [
    "9). Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in \n",
    "Logistic Regression. Print the best parameters and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c86e7744-507e-4510-be40-f9141c00fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5025a4e0-30ad-4e46-b064-c2872d2cb410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data():\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(1000, 10)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "    return X, y\n",
    "\n",
    "def randomized_search_logistic_regression():\n",
    "    X, y = generate_sample_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db8117ec-5246-415d-9dee-194375a17f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'l1_ratio': 0.8, 'C': 100.0}\n",
      "Best Accuracy: 0.9670329670329672\n",
      "Test Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "    param_dist = {\n",
    "        'C': np.logspace(-3, 3, 7),\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "        'l1_ratio': [0.2, 0.5, 0.8]\n",
    "    }\n",
    "\n",
    "    logreg = LogisticRegression(max_iter=10000)\n",
    "    random_search = RandomizedSearchCV(logreg, param_distributions=param_dist, n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "    print(\"Best Accuracy:\", random_search.best_score_)\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_test, random_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0292080-f9f2-44ea-817a-8e48d4043298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e64ada07-0936-4466-9d23-91ddba84ac14",
   "metadata": {},
   "source": [
    "10). Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b747ca1-79ee-438d-b342-a6afdc058edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-One Logistic Regression Accuracy: 0.325\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def one_vs_one_logistic_regression():\n",
    "    \"\"\"\n",
    "    Implements One-vs-One (OvO) Multiclass Logistic Regression and prints accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(1000, 10)\n",
    "    y = np.random.randint(0, 3, 1000)  # 3 classes\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "    ovo_logreg = OneVsOneClassifier(logreg)\n",
    "\n",
    "    ovo_logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = ovo_logreg.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"One-vs-One Logistic Regression Accuracy:\", accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    one_vs_one_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49429e49-85dc-4186-bcde-0ce8e5c71323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28611019-3275-444e-8563-a82d8e3b9b05",
   "metadata": {},
   "source": [
    "11). Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary \n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1625b70b-5e66-4171-8974-ad77b1aedc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def visualize_binary_confusion_matrix():\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model and visualizes the confusion matrix for binary classification.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2) \n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int) \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_binary_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3dca6-3a8f-4ebc-9bf2-ce747eb8e0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d387b9a4-69d0-4676-a2ce-9aec06dcd9e4",
   "metadata": {},
   "source": [
    "12). Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, \n",
    "Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3cbfdf38-a056-40e4-8125-82086cf5f406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.7\n",
      "F1-Score: 0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_logistic_regression():\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model and evaluates its performance using Precision, Recall, and F1-Score.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-Score:\", f1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccd2c7-3ebe-4484-844c-af349a13f3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9817dada-d91d-468c-baff-930a9218b140",
   "metadata": {},
   "source": [
    "13). Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to \n",
    "improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c1edec3c-c6e0-4ae1-a7fa-8d3c42df6cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression without Class Weights:\n",
      "Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      1.00      0.94       178\n",
      "         1.0       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.45      0.50      0.47       200\n",
      "weighted avg       0.79      0.89      0.84       200\n",
      "\n",
      "\n",
      "Logistic Regression with Class Weights:\n",
      "Accuracy: 0.53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.53      0.67       178\n",
      "         1.0       0.12      0.50      0.19        22\n",
      "\n",
      "    accuracy                           0.53       200\n",
      "   macro avg       0.51      0.52      0.43       200\n",
      "weighted avg       0.81      0.53      0.62       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def imbalanced_logistic_regression():\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model on imbalanced data and applies class weights.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    num_samples = 1000\n",
    "    minority_class_size = int(num_samples * 0.1) \n",
    "    majority_class_size = num_samples - minority_class_size\n",
    "\n",
    "    X = np.random.rand(num_samples, 2)\n",
    "    y = np.concatenate([np.zeros(majority_class_size), np.ones(minority_class_size)])\n",
    "\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg_no_weights = LogisticRegression()\n",
    "    logreg_no_weights.fit(X_train, y_train)\n",
    "    y_pred_no_weights = logreg_no_weights.predict(X_test)\n",
    "    print(\"Logistic Regression without Class Weights:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_no_weights))\n",
    "    print(classification_report(y_test, y_pred_no_weights))\n",
    "\n",
    "    logreg_weights = LogisticRegression(class_weight='balanced')\n",
    "    logreg_weights.fit(X_train, y_train)\n",
    "    y_pred_weights = logreg_weights.predict(X_test)\n",
    "    print(\"\\nLogistic Regression with Class Weights:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_weights))\n",
    "    print(classification_report(y_test, y_pred_weights))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    imbalanced_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5b68a-608a-4760-bc59-2abc6c81858d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d44ab71d-bb02-4920-9d9e-39c46e7df0e8",
   "metadata": {},
   "source": [
    "14). Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and \n",
    "evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a08d9-c614-4e50-8aa1-218aaddd0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def titanic_logistic_regression():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression on the Titanic dataset, handles missing values, and evaluates performance.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "        titanic_data = pd.read_csv(url)\n",
    "\n",
    "        # Handle missing values\n",
    "        # Impute missing 'Age' values with the median\n",
    "        imputer_age = SimpleImputer(strategy='median')\n",
    "        titanic_data['Age'] = imputer_age.fit_transform(titanic_data[['Age']])\n",
    "\n",
    "        # Impute missing 'Embarked' values with the most frequent value\n",
    "        imputer_embarked = SimpleImputer(strategy='most_frequent')\n",
    "        titanic_data['Embarked'] = imputer_embarked.fit_transform(titanic_data[['Embarked']])\n",
    "\n",
    "        # Encode categorical features\n",
    "        label_encoder = LabelEncoder()\n",
    "        titanic_data['Sex'] = label_encoder.fit_transform(titanic_data['Sex'])\n",
    "        titanic_data['Embarked'] = label_encoder.fit_transform(titanic_data['Embarked'])\n",
    "\n",
    "        # Select features and target variable\n",
    "        features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "        target = 'Survived'\n",
    "\n",
    "        X = titanic_data[features]\n",
    "        y = titanic_data[target]\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train Logistic Regression model\n",
    "        logreg = LogisticRegression(max_iter=1000)\n",
    "        logreg.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = logreg.predict(X_test)\n",
    "\n",
    "        # Evaluate performance\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    titanic_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb1911-0256-438f-981e-029b2528ec64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4be4ed32-2cab-428a-8142-effb1d77efdf",
   "metadata": {},
   "source": [
    "15). Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression \n",
    "model. Evaluate its accuracy and compare results with and without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "534b1d8d-84e0-46a2-9218-e0cfd88f171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 0.95\n",
      "Accuracy with scaling: 0.95\n",
      "\n",
      "Comparison:\n",
      "Accuracy difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def logistic_regression_with_scaling():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression with and without feature scaling (Standardization) and compares accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2) * 10 \n",
    "    y = (X[:, 0] + X[:, 1] > 10).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg_no_scaling = LogisticRegression()\n",
    "    logreg_no_scaling.fit(X_train, y_train)\n",
    "    y_pred_no_scaling = logreg_no_scaling.predict(X_test)\n",
    "    accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "    print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    logreg_scaled = LogisticRegression()\n",
    "    logreg_scaled.fit(X_train_scaled, y_train)\n",
    "    y_pred_scaled = logreg_scaled.predict(X_test_scaled)\n",
    "    accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "    print(\"Accuracy with scaling:\", accuracy_scaled)\n",
    "\n",
    "    # Compare results\n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"Accuracy difference: {accuracy_scaled - accuracy_no_scaling}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logistic_regression_with_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a1bb7-4471-4f13-bc95-a09c8173d4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26ae0441-f88c-4204-95cc-2d216b1f7f09",
   "metadata": {},
   "source": [
    "16). Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2a5df2ff-0617-4afd-afc2-20cf31a554d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def logistic_regression_roc_auc():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression and evaluates its performance using ROC-AUC score.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logistic_regression_roc_auc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467a49a-6815-46ea-bab9-26150b8e342f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb092b5c-0ee3-499c-ac32-20ceed5ff2d8",
   "metadata": {},
   "source": [
    "17). Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96f14078-fc60-433c-917c-9e490f7678c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=0.5: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def logistic_regression_custom_c():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression using a custom learning rate (C=0.5) and evaluates accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression(C=0.5)\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy with C=0.5:\", accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logistic_regression_custom_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132e810-114a-436f-9de4-fe73cee41a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eea50db7-3429-4014-b7cb-367f31ba3218",
   "metadata": {},
   "source": [
    "18). Write a Python program to train Logistic Regression and identify important features based on model \n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "81411346-0da2-455d-a742-3e0d0fc72241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Coefficients:\n",
      "Feature 1: 1.1227944430575938\n",
      "Feature 2: 3.5402152878985973\n",
      "Feature 3: -1.041608619627895\n",
      "Feature 4: -0.24632125481842648\n",
      "Feature 5: -0.3353753156261881\n",
      "\n",
      "Important Features (ranked by coefficient magnitude):\n",
      "Feature 2: Coefficient Magnitude = 3.5402152878985973\n",
      "Feature 1: Coefficient Magnitude = 1.1227944430575938\n",
      "Feature 3: Coefficient Magnitude = 1.041608619627895\n",
      "Feature 5: Coefficient Magnitude = 0.3353753156261881\n",
      "Feature 4: Coefficient Magnitude = 0.24632125481842648\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def identify_important_features():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression and identifies important features based on model coefficients.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    num_features = 5\n",
    "    X = np.random.rand(100, num_features)\n",
    "    y = (X[:, 0] + 2 * X[:, 1] - 0.5 * X[:, 2] > 1).astype(int)  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    coefficients = logreg.coef_[0] \n",
    "    print(\"Feature Coefficients:\")\n",
    "    for i, coef in enumerate(coefficients):\n",
    "        print(f\"Feature {i+1}: {coef}\")\n",
    "\n",
    "    feature_importance = [(abs(coef), i) for i, coef in enumerate(coefficients)]\n",
    "    feature_importance.sort(reverse=True)\n",
    "    print(\"\\nImportant Features (ranked by coefficient magnitude):\")\n",
    "    for abs_coef, feature_index in feature_importance:\n",
    "        print(f\"Feature {feature_index + 1}: Coefficient Magnitude = {abs_coef}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    identify_important_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce143cc-6896-47e6-898f-5a71015996c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772f13cb-965a-4888-b7ca-e8de3b1160bc",
   "metadata": {},
   "source": [
    "19). Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa \n",
    "Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fbb0ad5f-dfd2-44cb-bbea-d4b5f97199a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score: 0.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def logistic_regression_cohen_kappa():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression and evaluates its performance using Cohen's Kappa score.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Cohen's Kappa Score:\", kappa)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logistic_regression_cohen_kappa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe9aab-7131-429a-adec-7e8fb1605203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "082b8a68-adbb-4d7e-a7cc-79c2a02c86b9",
   "metadata": {},
   "source": [
    "20). Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary \n",
    "classificatio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "51a3b6f3-0498-48e3-93e9-247d005b1f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "def visualize_precision_recall_curve():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression and visualizes the Precision-Recall curve.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {auc_pr:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_precision_recall_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84a56f-41e3-4332-a2d0-7eb006ef0c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8b6eac-fe83-4faa-b984-9a752714155e",
   "metadata": {},
   "source": [
    "21). Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare \n",
    "their accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bc793d2c-5114-4a1b-b941-b4b8719c7a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with solver 'liblinear': 0.9\n",
      "Accuracy with solver 'saga': 0.9\n",
      "Accuracy with solver 'lbfgs': 0.85\n",
      "\n",
      "Accuracy Comparison:\n",
      "liblinear: 0.9\n",
      "saga: 0.9\n",
      "lbfgs: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compare_logistic_regression_solvers():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression with different solvers and compares their accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "    accuracies = {}\n",
    "\n",
    "    for solver in solvers:\n",
    "        try:\n",
    "            logreg = LogisticRegression(solver=solver, max_iter=1000)\n",
    "            logreg.fit(X_train, y_train)\n",
    "            y_pred = logreg.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            accuracies[solver] = accuracy\n",
    "            print(f\"Accuracy with solver '{solver}': {accuracy}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Solver '{solver}' encountered an error: {e}\")\n",
    "\n",
    "    print(\"\\nAccuracy Comparison:\")\n",
    "    for solver, accuracy in accuracies.items():\n",
    "        print(f\"{solver}: {accuracy}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_logistic_regression_solvers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4544f4-f786-4148-a795-4310ed1b3dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb66c03d-8bfd-4bd7-86c9-6ccf614e69af",
   "metadata": {},
   "source": [
    "22). Write a Python program to train Logistic Regression and evaluate its performance using Matthews \n",
    "Correlation Coefficient (MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "edaeda67-6dd6-4d9b-82ec-030991635b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.7337993857053428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def logistic_regression_mcc():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression and evaluates its performance using Matthews Correlation Coefficient (MCC).\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logistic_regression_mcc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e615ba-e94e-45a7-998e-c89b221231ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35592a6c-1ce5-4230-a066-e77786a04e1a",
   "metadata": {},
   "source": [
    "23). Write a Python program to train Logistic Regression on both raw and standardized data. Compare their \n",
    "accuracy to see the impact of feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c6fa0678-2f30-407b-9507-67ece770561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on raw data: 1.0\n",
      "Accuracy on standardized data: 1.0\n",
      "\n",
      "Accuracy Comparison:\n",
      "Accuracy difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compare_logistic_regression_scaling():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression on raw and standardized data and compares accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    X[:, 0] *= 10  # Scale feature 1\n",
    "    y = (X[:, 0] + X[:, 1] > 5).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg_raw = LogisticRegression(max_iter=1000)\n",
    "    logreg_raw.fit(X_train, y_train)\n",
    "    y_pred_raw = logreg_raw.predict(X_test)\n",
    "    accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "    print(\"Accuracy on raw data:\", accuracy_raw)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    logreg_scaled = LogisticRegression(max_iter=1000)\n",
    "    logreg_scaled.fit(X_train_scaled, y_train)\n",
    "    y_pred_scaled = logreg_scaled.predict(X_test_scaled)\n",
    "    accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "    print(\"Accuracy on standardized data:\", accuracy_scaled)\n",
    "\n",
    "    print(\"\\nAccuracy Comparison:\")\n",
    "    print(f\"Accuracy difference: {accuracy_scaled - accuracy_raw}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_logistic_regression_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38e6b3-d440-4747-bd17-6ad10838b86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a71594e-4648-470a-85a0-ef9195a23e36",
   "metadata": {},
   "source": [
    "24). Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using \n",
    "cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b324d469-f877-41ab-9570-9cfce9558ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 545.5594781168514\n",
      "Test Accuracy with Best C: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def find_optimal_c():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression and finds the optimal C using cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    param_grid = {'C': np.logspace(-4, 4, 20)} \n",
    "\n",
    "    logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_c = grid_search.best_params_['C']\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Best C:\", best_c)\n",
    "    print(\"Test Accuracy with Best C:\", accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_optimal_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdd580-a8e9-4452-bfa2-6640e196f830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e64defd-54d1-4ff8-b3d8-67ade7419136",
   "metadata": {},
   "source": [
    "25). Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to \n",
    "make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5e6b38ac-f15d-4016-be94-ddd7ec0c7018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'logistic_regression_model.joblib'\n",
      "Model loaded successfully.\n",
      "Accuracy with loaded model: 0.85\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_save_load_logistic_regression():\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression, saves the model using joblib, loads it, and makes predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(logreg, 'logistic_regression_model.joblib')\n",
    "    print(\"Model saved as 'logistic_regression_model.joblib'\")\n",
    "\n",
    "    loaded_model = joblib.load('logistic_regression_model.joblib')\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy with loaded model:\", accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_save_load_logistic_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
